{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [ 가변 길이의 텍스트 처리를 위한 모델 ]\n",
    "- 임베딩층 : nn.EmbeddingBag \n",
    "- 구현\n",
    "    * 입력 텍스트의 길이를 측정해서 길이만큼 추출 후 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] 모듈 로딩 및 데이터 준비<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KDP-35\\anaconda3\\envs\\NLP\\lib\\site-packages\\torchtext\\datasets\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\KDP-35\\anaconda3\\envs\\NLP\\lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "## 모듈 로딩\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torchtext.datasets import AG_NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 준비 \n",
    "SAVE_DIR = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] 데이터 로딩 및 확인<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KDP-35\\anaconda3\\envs\\NLP\\lib\\site-packages\\torchdata\\datapipes\\__init__.py:18: UserWarning: \n",
      "################################################################################\n",
      "WARNING!\n",
      "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
      "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
      "to learn more and leave feedback.\n",
      "################################################################################\n",
      "\n",
      "  deprecation_warning()\n"
     ]
    }
   ],
   "source": [
    "## pytorch의 torchtext의 내장 데이터셋 \n",
    "trainDS, testDS = AG_NEWS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [3] 데이터로더 생성 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KDP-35\\anaconda3\\envs\\NLP\\lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\KDP-35\\anaconda3\\envs\\NLP\\lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "## 관련 모듈 로딩\n",
    "from torch.utils.data import DataLoader                 ## 데이터셋, 데이터로더 관련 모듈\n",
    "\n",
    "## 토커나이저, 단어사전 관련 모듈\n",
    "from torchtext.data.utils import get_tokenizer          ## 토커나이저 인스턴스 추출\n",
    "from torchtext.vocab import build_vocab_from_iterator   ## 데이터셋에서 단어사전 생성 함수\n",
    "from nltk.corpus import stopwords                       ## 불용어 데이터셋\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3-1] 단어사전 생성 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOPWORDS : ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n"
     ]
    }
   ],
   "source": [
    "### ==> 특별 문자 토큰\n",
    "UNK, PAD  = '<UNK>',  '<PAD>'\n",
    "STOPWORDS = stopwords.words('english')\n",
    "print(f'STOPWORDS : {STOPWORDS[:10]}')\n",
    "\n",
    "### ==> 토커나이즈 생성\n",
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 토큰 제너레이터 함수 : 데이터 추출하여 토큰화 \n",
    "def yield_tokens(data_iter):\n",
    "    for label, news in data_iter:\n",
    "        # 라벨, 뉴스 --> 텍스트 토큰화\n",
    "        tokens = tokenizer(news)\n",
    "        # print(f'[tokens 1] => {len(tokens)}')\n",
    "        # tokens =[token for token in tokens if token not in STOPWORDS]\n",
    "        # print(f'[tokens 2] => {len(tokens)}')\n",
    "        yield tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocab] 95812개\n"
     ]
    }
   ],
   "source": [
    "### ===> 단어사전 생성\n",
    "vocab = build_vocab_from_iterator(yield_tokens(trainDS), \n",
    "                                  specials=[PAD, UNK])\n",
    "\n",
    "### ===> <PAD> 인덱스 0으로 설정\n",
    "vocab.set_default_index(vocab[PAD])\n",
    "\n",
    "print(f'[vocab] {len(vocab)}개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 텍스트 >>>> 정수 인코딩\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "\n",
    "### ===> 레이틀 >>> 정수 인코딩 (0~3) : 1~4\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3-2] 단어사전 생성 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 배치크기만큼 데이터 로딩 시 위치 지정 위한 설정 \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------\n",
    "## 함수기능 : 배치크기 만큼 데이터셋 로딩해서 토큰 + 텐서화 진행 후 반환\n",
    "## ----------------------------------------------------------------------\n",
    "def collate_batch(batch):\n",
    "    ## 라벨, 뉴스, 뉴스기사 시작 위치값 저장 변수\n",
    "    label_list, news_list, offsets = [], [], [0]\n",
    "\n",
    "    ## 1개씩 라벨과 뉴스 기사 추출\n",
    "    for label, news in batch:\n",
    "        ## 라벨 인코딩 후 추가 : 1 ~ 4 => 0 ~ 3\n",
    "        label_list.append(label_pipeline(label))\n",
    "\n",
    "        ## 뉴스 기사 인코딩 후 추가 \n",
    "        processed_news = torch.tensor(text_pipeline(news), dtype=torch.int64)\n",
    "        news_list.append(processed_news)\n",
    "\n",
    "        ## 다음 뉴스를 읽기 위한 위치값 정보\n",
    "        offsets.append(processed_news.size(0))\n",
    "        #print(f'news 토큰 수 => {processed_news.size(0)}개')\n",
    "\n",
    "    ## 배치 크기 만큼의 라벨 리스트 => 텐서화\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "\n",
    "    ## 배치 크기 만큼의 길이 위치값 => 텐서화 \n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    ## 배치 크기 만큼의 뉴스 기사 리스트 => 텐서화 \n",
    "    news_list = torch.cat(news_list)\n",
    "\n",
    "    return label_list.to(DEVICE), news_list.to(DEVICE), offsets.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset 종류 -------------------------------------------\n",
    "## => Map-style Dataset\n",
    "##    - 인덱스를 통한 랜덤 액세스\n",
    "##    - 전처리에 유용한 데이터셋\n",
    "\n",
    "## => Iterable-style Dataset\n",
    "##    - 순차적으로 하나씩 처리 \n",
    "##    - DataLoader를 통한 처리에 유용한 데이터셋\n",
    "## => Map DS ==> Iterable DS 변환 함수\n",
    "##    - to_map_style_dataset(Map_DS) => 결과 Iteralbe DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainDS : 114000개\n",
      "validDS : 6000개\n",
      "testDS  : 7600개\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "## Map Dataset ==> Iterable Dataset 변환\n",
    "trainDS = to_map_style_dataset(trainDS)\n",
    "testDS  = to_map_style_dataset(testDS)\n",
    "\n",
    "## 검증용 데이터셋위한 분할\n",
    "num_train = int(len(trainDS) * 0.95)\n",
    "num_valid = len(trainDS) - num_train\n",
    "\n",
    "trainDS, validDS = random_split( trainDS, [num_train, num_valid])\n",
    "\n",
    "print(f'trainDS : {len(trainDS)}개')\n",
    "print(f'validDS : {len(validDS)}개')\n",
    "print(f'testDS  : {len(testDS)}개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## => DataLoader 생성\n",
    "### ===> 학습용, 검증용, 테스트용 DataSet 준비 \n",
    "BATCH_SIZE = 5\n",
    "\n",
    "### 학습용, 검증용, 테스트용 Dataset, DataLoader 준비\n",
    "trainDL = DataLoader( trainDS, \n",
    "                      batch_size=BATCH_SIZE, \n",
    "                      shuffle=True, \n",
    "                      collate_fn=collate_batch )\n",
    "\n",
    "validDL  = DataLoader( validDS, \n",
    "                       batch_size=BATCH_SIZE,  \n",
    "                       shuffle=True,  \n",
    "                       collate_fn=collate_batch )\n",
    "                      \n",
    "testDL  = DataLoader( testDS, \n",
    "                      batch_size=BATCH_SIZE,  \n",
    "                      shuffle=True,  \n",
    "                      collate_fn=collate_batch )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] 모델 클래스 정의 및 설계 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 모듈로딩\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------------------------------------------------------------\n",
    "## 클래스이름 : TextModel\n",
    "## 부모클래스 : Module\n",
    "## 매개변수둘 : 단어사전 갯수, 임베딩 수, 분류_클래스 갯수 \n",
    "## -------------------------------------------------------------------------\n",
    "class TextModel(nn.Module):\n",
    "    ## 모델 층 정의 메서드 --------------------------------------\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        ## 고차원 ==> 저차원 \n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        ## 다중 분류 \n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        ## 초기 가중치 => self.메서드이름() : 같은 클래스에 존재하는 메서드 호출\n",
    "        self.init_weights()\n",
    "\n",
    "    ## 가중치 초기화 기능의 메서드 ---------------------------\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    ## 전방향 학습 메서드 -------------------------------------------\n",
    "    def forward(self, text, offsets):\n",
    "        ## 배치크기만큼 학습 데이터 \n",
    "        embedded = self.embedding(text, offsets)\n",
    "        # 다중 분류로 손실함수에서 softmax() 처리 \n",
    "        return self.fc(embedded)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5] 학습  <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> [5-1] 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 학습 설정\n",
    "EMBEDDING_DIM   = 64\n",
    "LR              = 0.01\n",
    "EPOCHS          = 10\n",
    "STEP_SIZE\t\t= 5\n",
    "\n",
    "NUM_CLASS       = len(set([label for (label, text) in trainDS]))\n",
    "VOCAB_SIZE      = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 학습 인스턴스 생성\n",
    "MODEL = TextModel(  VOCAB_SIZE, EMBEDDING_DIM, NUM_CLASS)\n",
    "MODEL.to(DEVICE)\n",
    "\n",
    "LOSS_FN   = nn.CrossEntropyLoss()\n",
    "OPTIMIZER = optim.Adam(MODEL.parameters(), lr=LR)\n",
    "SCHEDULER = StepLR(OPTIMIZER, STEP_SIZE, gamma=0.1)\t\t# 현재 LR * gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> [5-2]학습관련 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------------------------------------------------------------\n",
    "## 함수기능 : 학습데이터를 사용하여 학습 진행\n",
    "## 함수이름 : training\n",
    "## 매개변수 : 데이터로더\n",
    "## 결과반환 : 손실값, 모델성능값\n",
    "## -------------------------------------------------------------------------\n",
    "def training(dataloader):\n",
    "    ## 학습 모드 설정\n",
    "    MODEL.train()\n",
    "\n",
    "    ## 학습 손실과 점수 저장 \n",
    "    total_loss, total_acc = 0, 0\n",
    "    \n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "\n",
    "        OPTIMIZER.zero_grad()\n",
    "        pre  = MODEL(text, offsets)\n",
    "\n",
    "        loss = LOSS_FN(pre, label)\n",
    "        loss.backward()\n",
    "        ## gradient vanishing, gradient exploding 발생 => 방지 및 안정화 \n",
    "        ## - gradient가 일정 threshold를 넘어가면 clipping\n",
    "        ## - clipping: gradient의 L2norm(norm이지만 보통 L2 norm사용)으로 나눠주는 방식\n",
    "        torch.nn.utils.clip_grad_norm_(MODEL.parameters(), 0.1)\n",
    "        OPTIMIZER.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pre_label = nn.functional.softmax(pre, dim=1)\n",
    "        total_acc += (pre_label.argmax(dim=1) == label).sum().item()\n",
    "\n",
    "        if idx==5: break\n",
    "        \n",
    "    return total_loss/idx+1, total_acc/idx+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------------------------------------------------------------\n",
    "## 함수기능 : 검증데이터를 사용하여 학습 진행\n",
    "## 함수이름 : evaluate\n",
    "## 매개변수 : 데이터로더\n",
    "## 결과반환 : 손실값, 모델성능값\n",
    "## -------------------------------------------------------------------------\n",
    "def evaluate(dataloader):\n",
    "    MODEL.eval()\n",
    "    total_loss, total_acc = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            ## 추론 진행\n",
    "            pre = MODEL(text, offsets)\n",
    "            ## 손실 계산\n",
    "            loss = LOSS_FN(pre, label)\n",
    "\n",
    "            ## 손실 및 성능 평가\n",
    "            total_loss += loss.item()\n",
    "            total_acc += (pre.argmax(1) == label).sum().item()\n",
    "            if idx==5: break\n",
    "        \n",
    "    return total_loss/idx+1, total_acc/idx+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> [5-3]학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 및 모델 층별 상태값 즉, 파라미터 값 저장 경로\n",
    "MODEL_DIR  = '../models/'\n",
    "MODEL_FILE = 'AGNEWS_MODEL.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | train acc    3.000  | valid acc    2.600\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | train acc    2.800  | valid acc    3.000\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | train acc    2.800  | valid acc    3.000\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | train acc    3.000  | valid acc    3.400\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | train acc    4.000  | valid acc    4.400\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | train acc    4.000  | valid acc    5.000\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | train acc    4.400  | valid acc    4.600\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | train acc    4.400  | valid acc    4.200\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | train acc    4.400  | valid acc    4.800\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | train acc    3.800  | valid acc    4.200\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10  ## 임시\n",
    "# 모델 저장 기준\n",
    "MAX_ACC = 0.\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    \n",
    "    train_loss, train_acc = training(trainDL)\n",
    "    valid_loss, valid_acc = evaluate(validDL)\n",
    "    SCHEDULER.step()\n",
    "\n",
    "    print(\"-\" * 59)\n",
    "    print(f'| end of epoch {epoch:3d} | train acc {train_acc:8.3f}  | valid acc {valid_acc:8.3f}')\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "    ## 모델 저장 \n",
    "    if MAX_ACC < valid_acc : \n",
    "        torch.save(MODEL, MODEL_DIR+MODEL_FILE)\n",
    "        MAX_ACC = valid_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6] 모델 평가<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING......\t[RESULT] Acc : 4.600,  LOSS : 2.500\n"
     ]
    }
   ],
   "source": [
    "print('TESTING......', end='\\t')\n",
    "\n",
    "test_loss, test_acc = evaluate(testDL)\n",
    "\n",
    "print(f'[RESULT] Acc : {test_acc:.3f},  LOSS : {test_loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[7] 예측<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------------------------------------------------------------\n",
    "## 함수기능 : 사용자데이터를 예측\n",
    "## 함수이름 : predict\n",
    "## 매개변수 : 모델, 텍스트, 텍스트 전처리함수\n",
    "## 결과반환 : 예측값\n",
    "## -------------------------------------------------------------------------\n",
    "def predict(model, text, text_pipeline):\n",
    "    # 검증 모드 \n",
    "    model.eval()\n",
    "\n",
    "    # 예측 진행\n",
    "    with torch.no_grad():\n",
    "        # 토큰화 > 정수 변환  > 텐서\n",
    "        text = torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
    "      \n",
    "        # 예측 진행\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        # 예측 결과 반환\n",
    "        return output.argmax(1).item() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n",
    "\n",
    "\n",
    "### ==> 임의의 데이터 \n",
    "ex_text_str_01 = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "ex_text_str_02= \"As food prices continued to rise, consumer prices continued to rise in the 3% range for the second consecutive month.\\\n",
    "                According to the \\\"March Consumer Price Trend\\\" released by the National Statistical Office on the 2nd, the consumer price index \\\n",
    "                last month was 113.94 (2020 = 100), up 3.1% from the same month last year.\\\n",
    "                This year's consumer price growth rate increased again to 3.1% in February after recording 2.8% in January this year.\\\n",
    "                Prices of agricultural, livestock and fisheries products rose 11.7 percent year-on-year, up more than 11.4 percent from the previous month.\\\n",
    "                Among them, agricultural prices rose 20.5% year-on-year, marking the second consecutive month of increase of 20% following the previous month's 20.9%.\\\n",
    "                In particular, the price of apples rose 88.2 percent, which was larger than the previous month (71.0 percent), the largest increase ever since January 1980, \\\n",
    "                when statistics began to be compiled.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEWS => World\n",
      "NEWS => Sports\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(MODEL_DIR+MODEL_FILE)\n",
    "\n",
    "print(f\"NEWS => {ag_news_label[predict(model, ex_text_str_01, text_pipeline)]}\")\n",
    "print(f\"NEWS => {ag_news_label[predict(model, ex_text_str_02, text_pipeline)]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
